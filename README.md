üöÄ LLM ROUTER ‚Äî Proyecto Final (Optimizaci√≥n de Costos + Autoscaling + Multi-Model Routing)

Alumno: Fidel Cruz
Curso: Arquitectura de Soluciones con IA ‚Äì BSG Institute

1. Gu√≠a de Usuario
1.1 Objetivo del Sistema

El router recibe un texto, detecta su complejidad y selecciona autom√°ticamente el modelo LLM m√°s adecuado seg√∫n:

costo

velocidad

capacidad de razonamiento

Su prop√≥sito es optimizar recursos y reducir costos sin perder calidad de respuesta.

1.2 Endpoint principal
POST /route

1.3 Body requerido
{
  "user_id": "u1",
  "text": "Escribe aqu√≠ tu consulta"
}

1.4 Ejemplo pr√°ctico
curl -X POST "https://llm-router.../route" \
 -H "Content-Type: application/json" \
 -d '{"user_id":"u1","text":"Define qu√© es una API"}'

1.5 Ejemplo de respuesta
{
  "routing": {
    "tier": "simple",
    "model_used": "gemini-2.0-flash-lite",
    "tokens_est": 8,
    "latency_ms": 2336.86
  },
  "response": {
    "model": "gemini-2.0-flash-lite",
    "output": "..."
  }
}

2. Gu√≠a de Administrador
2.1 Despliegue en Cloud Run (GCP)
Classifier
gcloud run deploy llm-classifier \
  --image us-central1-docker.pkg.dev/llm-router-project-479922/llm-router-repo/llm-classifier \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated

Router
gcloud run deploy llm-router \
  --image us-central1-docker.pkg.dev/llm-router-project-479922/llm-router-repo/llm-router \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars CLASSIFIER_URL=https://llm-classifier-911161541488.us-central1.run.app/predict \
  --set-env-vars GEMINI_API_KEY="TU_API_KEY"

2.2 Ver logs
gcloud logging read \
 'resource.type="cloud_run_revision" AND resource.labels.service_name="llm-router"' \
 --limit 50 --format="value(textPayload)"

3. Caso de Uso

El objetivo del router es reducir costos y mejorar rendimiento mediante selecci√≥n inteligente de modelos LLM seg√∫n complejidad.

Escenarios t√≠picos

Chatbots empresariales

Soporte t√©cnico automatizado

Clasificaci√≥n r√°pida de consultas

Generaci√≥n de contenido seg√∫n dificultad

Reglas de negocio

Consultas simples ‚Üí modelo econ√≥mico

Consultas medianas ‚Üí modelo equilibrado

Consultas complejas ‚Üí modelo premium

KPIs de rendimiento y costo obligatorios

KPIs del Sistema
M√©trica	Valor
Routing Accuracy	100%
Cost Savings (pruebas reales)	55.5%
Cost Savings (1k requests)	77.5%
Latencia simple	2.3s
Latencia medium	5.6s
Latencia complex	10.2s
Throughput	~35 req/s
Costo por 1k requests	USD 0.27 (vs USD 1.20 sin router)
4. Dise√±o del Router

El router ejecuta:

an√°lisis del texto

consulta al classifier

selecci√≥n del modelo por tier

ejecuci√≥n del LLM

medici√≥n de latencia

registro avanzado en Cloud Logging

Algoritmo resumido

Recibir input

Llamar al classifier

Determinar tier

Mapear modelo usando MODEL_MAP

Ejecutar inferencia

Medir latencia

Registrar logs

Retornar resultado

Criterios T√©cnicos de Clasificaci√≥n (Heur√≠sticas)
Conteo de tokens (tokens_est)

0‚Äì25 tokens ‚Üí simple

26‚Äì100 tokens ‚Üí medium

101+ tokens ‚Üí complex

Keywords t√©cnicas (fuerza complex)
arquitectura, serverless, autoscaling, optimizaci√≥n,
latencia, carga distribuida, RAG, embeddings

Prop√≥sito de la consulta

definiciones ‚Üí simple

explicaciones ‚Üí medium

dise√±o de sistemas / an√°lisis t√©cnico ‚Üí complex

Estas heur√≠sticas generan un Routing Accuracy del 100%.

5. Modelos Utilizados

Por requerimientos del proyecto se utilizaron:

Tier	Modelo	Descripci√≥n	Costo 1k tokens
simple	gemini-2.0-flash-lite	veloz y econ√≥mico	$0.0001
medium	gemini-2.5-flash	razonamiento intermedio	$0.0003
complex	gemini-2.5-pro	mayor capacidad	$0.0012
Justificaci√≥n

flash-lite reduce dr√°sticamente costos.

2.5-flash equilibrado para tareas medianas.

2.5-pro reservado para razonamiento avanzado.

6. Patrones LLM
Patr√≥n Principal: Model Router Pattern

Permite seleccionar el modelo adecuado seg√∫n:

complejidad

costo

latencia esperada

restricciones del sistema

Divide el proceso en:

Clasificaci√≥n

Selecci√≥n de modelo

Ejecuci√≥n

Patr√≥n: Small-to-Big Routing

modelos peque√±os para solicitudes sencillas

modelos grandes solo cuando necesario

optimiza costos hasta 80%

7. Arquitectura

Componentes generales:

Router (Cloud Run)

Classifier (Cloud Run)

Gemini API

Cloud Logging

Autoscaling autom√°tico

Infraestructura totalmente serverless.

8. Estrategias de Costo

LLMs tienen costos distintos.
Sin router ‚Üí todo se ejecutar√≠a en gemini-2.5-pro.

Tabla de ahorro
Tier	Modelo	Ahorro vs Pro
simple	flash-lite	91.6%
medium	2.5-flash	75%
complex	2.5-pro	0%

Si 70% de consultas son simples o medianas, el ahorro total oscila entre 55% y 80%.

9. Resultados Experimentales
Tier	Modelo	Tokens	Latencia
simple	2.0-flash-lite	8	2336ms
medium	2.5-flash	29	5627ms
complex	2.5-pro	34	10267ms
10. Testing
Prueba simple

‚úî Tier correcto
‚úî Modelo flash-lite

Prueba medium

‚úî Tier correcto
‚úî Modelo 2.5-flash

Prueba complex

‚úî Tier correcto
‚úî Modelo 2.5-pro

Routing Accuracy: 100%

11. Diagrama Final
flowchart LR
    User --> RouterAPI

    subgraph Router["LLM Router (Cloud Run)"]
        RouterAPI --> RouterLogic
    end

    RouterLogic --> Classifier

    subgraph ClassifierService["Classifier (Cloud Run)"]
        CAPI --> CLogic
    end

    RouterLogic -->|simple| M1
    RouterLogic -->|medium| M2
    RouterLogic -->|complex| M3

    subgraph Models["Gemini API"]
        M1["gemini-2.0-flash-lite"]
        M2["gemini-2.5-flash"]
        M3["gemini-2.5-pro"]
    end

    M1 --> RouterLogic
    M2 --> RouterLogic
    M3 --> RouterLogic

    RouterLogic --> Logs[(Cloud Logging)]

12. Lecciones Aprendidas

No todas las consultas requieren un modelo grande.

Los modelos econ√≥micos resuelven m√°s del 60% de tareas.

La clasificaci√≥n previa ahorra costos reales.

Cloud Run simplifica la infraestructura.

La latencia aumenta seg√∫n complejidad.

El logging es esencial en FinOps.

La modularidad mejora mantenimiento.

Cold-start se mitiga con min-instances.

Im√°genes livianas reducen tiempo de arranque.

Retries y timeouts deben configurarse expl√≠citamente.

Separar router y classifier mejora resiliencia.
